{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing all tweets from the csv's\n",
    "tweet_folder = r'/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data/tweets'\n",
    "clean_tweet_folder = r'/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data/clean_tweets'\n",
    "account_files = os.listdir(tweet_folder)\n",
    "account_files = [a for a in account_files if a[0] != \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting row count for tweets\n",
    "row_count = 0\n",
    "for f_name in account_files :\n",
    "    with open(tweet_folder + \"/\" + f_name) as infile :\n",
    "        next(infile)\n",
    "        \n",
    "        for line in infile.readlines() :\n",
    "            row_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cleaning on one file\n",
    "f_name = account_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding where errors live\n",
    "with open(tweet_folder + \"/\" + f_name) as infile :\n",
    "    with open(clean_tweet_folder + \"/\" + f_name[:-4] + \"_outfile.csv\", 'w') as outfile :\n",
    "        next(infile)\n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip().split(',')\n",
    "            out_line = [\"\"]*3\n",
    "            out_line[0] = line[0]\n",
    "            out_line[1] = line[1]\n",
    "            if len(line) > 3 :\n",
    "                \n",
    "                out_line[2] = ','.join(line[2:])\n",
    "                #break\n",
    "            else :\n",
    "                out_line[2] = line[2]\n",
    "                \n",
    "                \n",
    "            outfile.write('\\t'.join(out_line) + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex to filter for tweet ID number \n",
    "id_regex = re.compile(r'^[0-9]{10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting how many rows start with tweet ID and how many do not to determine the extent of issue\n",
    "id_rows = 0\n",
    "non_id_rows = 0\n",
    "\n",
    "with open(tweet_folder + \"/\" + f_name) as infile :\n",
    "    next(infile)\n",
    "    for line in infile.readlines() :\n",
    "        if id_regex.search(line) :\n",
    "            id_rows += 1\n",
    "        else :\n",
    "            non_id_rows += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_rows) \n",
    "print(non_id_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't fuck up this code\n",
    "headers = \"id created_at tweet_text\".split()\n",
    "\n",
    "for f_name in account_files :\n",
    "    print(\"Now working on \" + f_name)\n",
    "    row_count = 0 \n",
    "    \n",
    "    out_line = [\"\"]*3\n",
    "\n",
    "    with open(tweet_folder + \"/\" + f_name) as infile :\n",
    "        with open(clean_tweet_folder + \"/\" + f_name[:-4] + \"_outfile.csv\", 'w') as outfile :\n",
    "            outfile.write(\"\\t\".join(headers) + \"\\n\")\n",
    "            next(infile)\n",
    "\n",
    "            for idx, line in enumerate(infile.readlines()) :    \n",
    "                has_id = id_regex.search(line)\n",
    "\n",
    "                if has_id :\n",
    "\n",
    "                    # write out completed line\n",
    "                    if out_line[0] != \"\" :\n",
    "                        outfile.write('\\t'.join(out_line) + '\\n')\n",
    "                        row_count += 1\n",
    "\n",
    "                    out_line = [\"\"]*3\n",
    "\n",
    "                    line = line.strip().split(',')\n",
    "                    out_line[0] = line[0]\n",
    "                    out_line[1] = line[1]            \n",
    "\n",
    "                    if len(line) > 3 :\n",
    "                        out_line[2] = ','.join(line[2:])\n",
    "\n",
    "                    else :\n",
    "                        out_line[2] = line[2] \n",
    "                else :\n",
    "                    out_line[2] = out_line[2] + \" \" + line.strip()\n",
    "            \n",
    "            if len(out_line) == 3 : \n",
    "                outfile.write('\\t'.join(out_line) + '\\n')\n",
    "                row_count += 1\n",
    "            \n",
    "    print(f\"Wrote {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file = \"all_clean_tweets.txt\"\n",
    "combined_location = \"/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data\"\n",
    "\n",
    "clean_files = os.listdir(clean_tweet_folder)\n",
    "headers = [\"organization\"] + headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining tweets with organization name\n",
    "with open(combined_location + \"/\" + combined_file,'w',encoding=\"UTF-8\") as outfile :\n",
    "    outfile.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    \n",
    "    for ff in clean_files :\n",
    "        if ff[0] != \".\" :\n",
    "            \n",
    "            # dealing with underscores in handles\n",
    "            file_name_pieces = ff.split(\"_\")\n",
    "            \n",
    "            if len(file_name_pieces) == 3 :\n",
    "                org = file_name_pieces[0]\n",
    "            else :\n",
    "                org = \"_\".join(file_name_pieces[:2])\n",
    "            \n",
    "            with open(clean_tweet_folder + \"/\" + ff,encoding=\"UTF-8\") as infile :\n",
    "                next(infile)\n",
    "                for line in infile.readlines() :\n",
    "                    line = org + \"\\t\" + line\n",
    "                    outfile.write(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell was to count who had the most tweets to help with analysis of importance,\n",
    "# however, tweet pull was capped at 3247 tweets\n",
    "# so for analysis, used number of followers instead\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "org_counts = []\n",
    "\n",
    "with open(combined_location + \"/\" + combined_file,'r',encoding=\"UTF-8\") as infile :\n",
    "    next(infile)\n",
    "    for row in infile :\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        \n",
    "        org_counts.append(row[0])\n",
    "    \n",
    "\n",
    "Counter(org_counts).most_common(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
