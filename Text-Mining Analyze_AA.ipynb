{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Twitter Descriptions of Land Trust Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the descriptions of the land trust Twitter accounts\n",
    "file_location = \"/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data/\"\n",
    "file_name = \"20191130_landtrusts_userobj.txt\"\n",
    "\n",
    "descs = []\n",
    "with open(file_location + file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 6 has the description\n",
    "        if len(line) >= 7 : # sometimes there is not a description\n",
    "            descs.extend(line[6].split())\n",
    "        \n",
    "        # add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_location + file_name,'r') as ifile :\n",
    "    print(ifile.readline())\n",
    "    print(ifile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of descriptions we have\n",
    "len(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most common words in the descriptions\n",
    "fd = FreqDist(descs)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean this list to exclude stop words and only alphanumeric words\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_sp = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "\n",
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to lowercase\n",
    "        * stopwords removed\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.lower() for w in text if w.isalpha()]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords_sp]\n",
    "    return(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the clean list\n",
    "descs_clean_LT = clean_list(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New clean 20 most common words in the descriptions\n",
    "fd_LT = FreqDist(descs_clean_LT)\n",
    "fd_LT.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt_desc = Text(descs_clean_LT)\n",
    "lt_desc.concordance('land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_desc.concordance('conservation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_desc.similar('land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_desc.similar('conservation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk work around for .collection() issue\n",
    "print('; '.join(lt_desc.collocation_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words used in descriptions sorted alphabetically\n",
    "sorted(set(lt_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words in descriptions\n",
    "len(set(lt_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(lt_desc))/len(lt_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the locations of the land trust Twitter accounts\n",
    "file_location = \"/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data/\"\n",
    "file_name = \"20191130_landtrusts_userobj.txt\"\n",
    "\n",
    "geo = []\n",
    "with open(file_location + file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 3 has the location\n",
    "        if len(line) >= 4 : # sometimes there is not a location\n",
    "            geo.extend(line[3].split())\n",
    "        \n",
    "        # add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited function to capture states in location spot\n",
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to uppercase to catch states\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.upper() for w in text if w.isalpha()]\n",
    "    return(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the clean list\n",
    "geo_clean = clean_list(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 locations of organizations \n",
    "fd = FreqDist(geo_clean)\n",
    "fd.most_common(10)\n",
    "\n",
    "#JOHN - when doing more (10, 20, etc) the locations included things like \"NEW\", \"SAN\"\n",
    "# which are not states. Is this more of a user input issue or a cleaning issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Tweets of Land Trust Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the tweets of the land trust Twitter accounts\n",
    "file_location = \"/Users/hannahleonard/Documents/UM/2019_Fall/ADA/AA_Project/Data/\"\n",
    "file_name = \"all_clean_tweets.txt\"\n",
    "\n",
    "tweets = []\n",
    "with open(file_location + file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 3 has the tweet\n",
    "        if len(line) >= 4 : # in case there isn't a tweet\n",
    "            tweets.extend(line[3].split())\n",
    "        # add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_location + file_name,'r') as ifile :\n",
    "    print(ifile.readline())\n",
    "    print(ifile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most common words in the tweets\n",
    "fd = FreqDist(tweets)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean this list to exclude stop words and only alphanumeric words\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_sp = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "\n",
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to lowercase\n",
    "        * stopwords removed\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.lower() for w in text if w.isalpha()]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords_sp]\n",
    "    return(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the clean list\n",
    "clean_tweets = clean_list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New clean 20 most common words in the tweets\n",
    "fd_CT = FreqDist(clean_tweets)\n",
    "fd_CT.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lt_tweet = Text(clean_tweets)\n",
    "lt_tweet.concordance('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_tweet.concordance('conservation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_tweet.common_contexts(['conservation','legacy','money'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_tweet.similar('land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_tweet.similar('legacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk work around for .collection() issue\n",
    "print('; '.join(lt_tweet.collocation_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words used in descriptions sorted alphabetically\n",
    "sorted(set(lt_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words in descriptions\n",
    "len(set(lt_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(lt_tweet))/len(lt_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
